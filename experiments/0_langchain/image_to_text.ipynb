{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image to Text\n",
    "Using Langchain with image input.   \n",
    "See this [Medium blog](https://nayakpplaban.medium.com/ask-questions-to-your-images-using-langchain-and-python-1aeb30f38751) for precise information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries\n",
    "Following libraries are required.\n",
    "- openai\n",
    "- langchain\n",
    "- transformers\n",
    "- tabulate\n",
    "- timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import BaseTool\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration, DetrImageProcessor, DetrForObjectDetection\n",
    "from PIL import Image\n",
    "import torch\n",
    "#\n",
    "import os\n",
    "from tempfile import NamedTemporaryFile\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains.conversation.memory import ConversationBufferWindowMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define tools & functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageCaptionTool(BaseTool):\n",
    "    name = \"Image captioner\"\n",
    "    description = \"Use this tool when given the path to an image that you would like to be described. \" \\\n",
    "                  \"It will return a simple caption describing the image.\"\n",
    "\n",
    "    def _run(self, img_path):\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        model_name = \"Salesforce/blip-image-captioning-large\"\n",
    "        device = \"cpu\"  # cuda\n",
    "\n",
    "        processor = BlipProcessor.from_pretrained(model_name)\n",
    "        model = BlipForConditionalGeneration.from_pretrained(model_name).to(device)\n",
    "\n",
    "        inputs = processor(image, return_tensors='pt').to(device)\n",
    "        output = model.generate(**inputs, max_new_tokens=20)\n",
    "\n",
    "        caption = processor.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "        return caption\n",
    "\n",
    "    def _arun(self, query: str):\n",
    "        raise NotImplementedError(\"This tool does not support async\")\n",
    "\n",
    "\n",
    "class ObjectDetectionTool(BaseTool):\n",
    "    name = \"Object detector\"\n",
    "    description = \"Use this tool when given the path to an image that you would like to detect objects. \" \\\n",
    "                  \"It will return a list of all detected objects. Each element in the list in the format: \" \\\n",
    "                  \"[x1, y1, x2, y2] class_name confidence_score.\"\n",
    "\n",
    "    def _run(self, img_path):\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        processor = DetrImageProcessor.from_pretrained(\"facebook/detr-resnet-50\")\n",
    "        model = DetrForObjectDetection.from_pretrained(\"facebook/detr-resnet-50\")\n",
    "\n",
    "        inputs = processor(images=image, return_tensors=\"pt\")\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "        # convert outputs (bounding boxes and class logits) to COCO API\n",
    "        # let's only keep detections with score > 0.9\n",
    "        target_sizes = torch.tensor([image.size[::-1]])\n",
    "        results = processor.post_process_object_detection(outputs, target_sizes=target_sizes, threshold=0.9)[0]\n",
    "\n",
    "        detections = \"\"\n",
    "        for score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n",
    "            detections += '[{}, {}, {}, {}]'.format(int(box[0]), int(box[1]), int(box[2]), int(box[3]))\n",
    "            detections += ' {}'.format(model.config.id2label[int(label)])\n",
    "            detections += ' {}\\n'.format(float(score))\n",
    "\n",
    "        return detections\n",
    "\n",
    "    def _arun(self, query: str):\n",
    "        raise NotImplementedError(\"This tool does not support async\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_caption(image_path):\n",
    "    \"\"\"\n",
    "    Generates a short caption for the provided image.\n",
    "\n",
    "    Args:\n",
    "        image_path (str): The path to the image file.\n",
    "\n",
    "    Returns:\n",
    "        str: A string representing the caption for the image.\n",
    "    \"\"\"\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "    model_name = \"Salesforce/blip-image-captioning-large\"\n",
    "    device = \"cpu\"  # cuda\n",
    "\n",
    "    processor = BlipProcessor.from_pretrained(model_name)\n",
    "    model = BlipForConditionalGeneration.from_pretrained(model_name).to(device)\n",
    "\n",
    "    inputs = processor(image, return_tensors='pt').to(device)\n",
    "    output = model.generate(**inputs, max_new_tokens=20)\n",
    "\n",
    "    caption = processor.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    return caption\n",
    "\n",
    "\n",
    "def detect_objects(image_path):\n",
    "    \"\"\"\n",
    "    Detects objects in the provided image.\n",
    "\n",
    "    Args:\n",
    "        image_path (str): The path to the image file.\n",
    "\n",
    "    Returns:\n",
    "        str: A string with all the detected objects. Each object as '[x1, x2, y1, y2, class_name, confindence_score]'.\n",
    "    \"\"\"\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "    processor = DetrImageProcessor.from_pretrained(\"facebook/detr-resnet-50\")\n",
    "    model = DetrForObjectDetection.from_pretrained(\"facebook/detr-resnet-50\")\n",
    "\n",
    "    inputs = processor(images=image, return_tensors=\"pt\")\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "    # convert outputs (bounding boxes and class logits) to COCO API\n",
    "    # let's only keep detections with score > 0.9\n",
    "    target_sizes = torch.tensor([image.size[::-1]])\n",
    "    results = processor.post_process_object_detection(outputs, target_sizes=target_sizes, threshold=0.9)[0]\n",
    "\n",
    "    detections = \"\"\n",
    "    for score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n",
    "        detections += '[{}, {}, {}, {}]'.format(int(box[0]), int(box[1]), int(box[2]), int(box[3]))\n",
    "        detections += ' {}'.format(model.config.id2label[int(label)])\n",
    "        detections += ' {}\\n'.format(float(score))\n",
    "\n",
    "    return detections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from getpass import getpass\n",
    "#set the openai_api_key\n",
    "# openai_api_key = getpass()\n",
    "openai_api_key = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize the gent\n",
    "tools = [ImageCaptionTool(), ObjectDetectionTool()]\n",
    "\n",
    "conversational_memory = ConversationBufferWindowMemory(\n",
    "    memory_key='chat_history',\n",
    "    k=5,\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    openai_api_key= openai_api_key,\n",
    "    temperature=0,\n",
    "    model_name=\"gpt-3.5-turbo\"\n",
    ")\n",
    "\n",
    "agent = initialize_agent(\n",
    "    agent=\"chat-conversational-react-description\",\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    max_iterations=5,\n",
    "    verbose=True,\n",
    "    memory=conversational_memory,\n",
    "    early_stopping_method='generate'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  292k  100  292k    0     0   112k      0  0:00:02  0:00:02 --:--:--  113k\n"
     ]
    }
   ],
   "source": [
    "#download the image\n",
    "# !wget https://www.smartcitiesworld.net/AcuCustom/Sitename/DAM/019/Parsons_PR.jpg\n",
    "!curl -OL https://www.smartcitiesworld.net/AcuCustom/Sitename/DAM/019/Parsons_PR.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ask questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m{\n",
      "    \"action\": \"Image captioner\",\n",
      "    \"action_input\": \"/Users/naoki/github/Rutilea/experiments/0_langchain/content/Parsons_PR.jpg\"\n",
      "}\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mcars are driving down the street in traffic at a green light\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m{\n",
      "    \"action\": \"Final Answer\",\n",
      "    \"action_input\": \"The atmosphere of the image is a busy street with cars driving down the road in traffic at a green light.\"\n",
      "}\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "The atmosphere of the image is a busy street with cars driving down the road in traffic at a green light.\n"
     ]
    }
   ],
   "source": [
    "image_path = \"/Users/naoki/github/Rutilea/experiments/0_langchain/content/Parsons_PR.jpg\"\n",
    "# user_question = \"generate a caption for this iamge?\"\n",
    "user_question = \"Tell me the atmosphere of this image.\"\n",
    "response = agent.run(f'{user_question}, this is the image path: {image_path}')\n",
    "print(response)\n",
    "\n",
    "# >Question 1 Entering new AgentExecutor chain...\n",
    "# {\n",
    "#     \"action\": \"Image captioner\",\n",
    "#     \"action_input\": \"/content/Parsons_PR.jpg\"\n",
    "# }\n",
    "# Observation: cars are driving down the street in traffic at a green light\n",
    "# Thought:{\n",
    "#     \"action\": \"Final Answer\",\n",
    "#     \"action_input\": \"The image shows cars driving down the street in traffic at a green light.\"\n",
    "# }\n",
    "\n",
    "# > Finished chain.\n",
    "# response: The image shows cars driving down the street in traffic at a green light\n",
    "# ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"/content/Parsons_PR.jpg\"\n",
    "user_question = \"Please tell me what are the items present in the image.\"\n",
    "response = agent.run(f'{user_question}, this is the image path: {image_path}')\n",
    "print(response)\n",
    "\n",
    "# > Entering new AgentExecutor chain...\n",
    "# {\n",
    "#     \"action\": \"Object detector\",\n",
    "#     \"action_input\": \"/content/Parsons_PR.jpg\"\n",
    "# }\n",
    "# Observation: [518, 40, 582, 110] car 0.9300937652587891\n",
    "# [188, 381, 311, 469] car 0.9253759384155273\n",
    "# [1068, 223, 1104, 342] person 0.987162172794342\n",
    "# [828, 233, 949, 329] car 0.9450376629829407\n",
    "# [1076, 263, 1106, 347] bicycle 0.9070376753807068\n",
    "# [635, 71, 713, 135] car 0.921174168586731\n",
    "# [0, 433, 100, 603] car 0.9781951308250427\n",
    "# [151, 747, 339, 799] car 0.9839044809341431\n",
    "# [389, 267, 493, 367] car 0.9801359176635742\n",
    "# [192, 478, 341, 633] car 0.995318591594696\n",
    "# [578, 117, 828, 550] traffic light 0.9860804677009583\n",
    "# [802, 666, 1028, 798] car 0.982887327671051\n",
    "# [0, 639, 84, 799] car 0.9630037546157837\n",
    "# [1057, 608, 1199, 766] car 0.9652799367904663\n",
    "# [988, 218, 1031, 347] person 0.9471640586853027\n",
    "# [751, 524, 909, 675] car 0.9911800026893616\n",
    "# [489, 560, 670, 749] car 0.9970000386238098\n",
    "\n",
    "# Thought:{\n",
    "#     \"action\": \"Final Answer\",\n",
    "#     \"action_input\": \"The objects present in the image are: car, car, person, car, bicycle, car, car, car, car, car, traffic light, car, car, car, person, car, car.\"\n",
    "# }\n",
    "\n",
    "# > Finished chain.\n",
    "\n",
    "# response: \"The objects present in the image are: car, car, person, car, bicycle, ca\n",
    "# , car, car, car, car, traffic light, car, car, car, person, car, car.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"/content/Parsons_PR.jpg\"\n",
    "user_question = \"Please tell me the bounding boxes of all detected objects in the image.\"\n",
    "response = agent.run(f'{user_question}, this is the image path: {image_ath}')\n",
    "print(response)\n",
    "\n",
    "# > Entering new AgentExecutor chain...\n",
    "# {\n",
    "#     \"action\": \"Object detector\",\n",
    "#     \"action_input\": \"/content/Parsons_PR.jpg\"\n",
    "# }\n",
    "# Observation: [518, 40, 582, 110] car 0.9300937652587891\n",
    "# [188, 381, 311, 469] car 0.9253759384155273\n",
    "# [1068, 223, 1104, 342] person 0.987162172794342\n",
    "# [828, 233, 949, 329] car 0.9450376629829407\n",
    "# [1076, 263, 1106, 347] bicycle 0.9070376753807068\n",
    "# [635, 71, 713, 135] car 0.921174168586731\n",
    "# [0, 433, 100, 603] car 0.9781951308250427\n",
    "# [151, 747, 339, 799] car 0.9839044809341431\n",
    "# [389, 267, 493, 367] car 0.9801359176635742\n",
    "# [192, 478, 341, 633] car 0.995318591594696\n",
    "# [578, 117, 828, 550] traffic light 0.9860804677009583\n",
    "# [802, 666, 1028, 798] car 0.982887327671051\n",
    "# [0, 639, 84, 799] car 0.9630037546157837\n",
    "# [1057, 608, 1199, 766] car 0.9652799367904663\n",
    "# [988, 218, 1031, 347] person 0.9471640586853027\n",
    "# [751, 524, 909, 675] car 0.9911800026893616\n",
    "# [489, 560, 670, 749] car 0.9970000386238098\n",
    "\n",
    "# Thought:{\n",
    "#     \"action\": \"Final Answer\",\n",
    "#     \"action_input\": \"The detected objects in the image are: \\n[518, 40, 582, 110] car \\n[188, 381, 311, 469] car \\n[1068, 223, 1104, 342] person \\n[828, 233, 949, 329] car \\n[1076, 263, 1106, 347] bicycle \\n[635, 71, 713, 135] car \\n[0, 433, 100, 603] car \\n[151, 747, 339, 799] car \\n[389, 267, 493, 367] car \\n[192, 478, 341, 633] car \\n[578, 117, 828, 550] traffic light \\n[802, 666, 1028, 798] car \\n[0, 639, 84, 799] car \\n[1057, 608, 1199, 766] car \\n[988, 218, 1031, 347] person \\n[751, 524, 909, 675] car \\n[489, 560, 670, 749] car\"\n",
    "# }\n",
    "\n",
    "# > Finished chain.\n",
    "# response: The detected objects in the image are: \\n[518, 40, 582, 110] car \\n[188, 381, 311, 469] car \\n[1068, 223, 1104, 342] person \\n[828, 233, 949, 329] car \\n[1076, 263, 1106, 347] bicycle \\n[635, 71, 713, 135] car \\n[0, 433, 100, 603] car \\n[151, 747, 339, 799] car \\n[389, 267, 493, 367] car \\n[192, 478, 341, 633] car \\n[578, 117, 828, 550] traffic light \\n[802, 666, 1028, 798] car \\n[0, 639, 84, 799] car \\n[1057, 608, 1199, 766] car \\n[988, 218, 1031, 347] person \\n[751, 524, 909, 675] car \\n[489, 560, 670, 749] car\"\n",
    "# }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Rutilea",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
